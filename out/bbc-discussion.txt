11.
(a) 
    The metric that is best suited to this dataset/task is Accuracy.Accuracy is the percentage of instances of the test set the algorithm
    correctly classifies. The instances of each of the 5 classes have been already categorized and provided to us.
    When we run the counter on step 2, we accurately predicted the total instances of each of the classes.Based on the barchart we generated,
    we can see the distribution of the instances of each of the classes. Sport has the highest record of instances.
    Business has almost the same number of instances as Sports. Number of instances of Tech and Politics are close to each other.
    Lastly, Entertainment has the lowest record of instances. The classes seemed somewhat balanced based on this pre-analysis.
    The second-best metrics I would choose for predicting classes is precision. Although, accuracy seemed consistent across all the evaluations,
    precision seemed bias to the classes business and sport, recall to politics and sport and F1-score shows a good distribution that even matched accuracy
    and other metrics in predicting the result, but still affected by both of them. However, precision matches the distribution was
    initially foreseen in the histogram in step 2.
(b)
    The performance of the steps (8-10) are about the same than those of step 7 because
    instances of each of the 5 classes have been already categorized and provided to us.
    If we look at the precisions, recall and f1-measure of step 7 and step 8-10,
    we can notice that their values are very close
    to 1 even though we altered the Multi-nominalNB default value and smoothing on the test set.
    Values close to 1 indicates balance.
    We can also see that the accuracy, macro avg and weighted avg and more are mostly of the same value
    and very close to 1 for the mentioned steps above.
    Sports and Business had the most precision for the steps mention (7-10) and Sports and Politics the highest recall.
    All these metrics helped us accurately test our model and its predictions.
    Changing smoothing across all the tests show bizarre fluctuations among the classes' metrics, while accuracy remained the same.
    And that's mostly because calculating accuracy close to the macro and weighted average, and precision relies on the predictions mostly,
    while recall uses the expected values, and f1-score uses the value of both of them.
