{"metadata":{"colab":{"name":"Copy of Mini_Project_1.ipynb","provenance":[{"file_id":"129NFVM56JTk53RtumEg1uq1Gqv1nD585","timestamp":1632259093165}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Step 1: Load the corpus using load files and make sure you set the encoding to latin1. (Task 1.3)","metadata":{"id":"OVuTabWXoTss"}},{"cell_type":"markdown","source":"### Get and group the data","metadata":{"id":"nytdwd5yqlEy"}},{"cell_type":"code","source":"import sklearn.datasets\n\n\ndef load_files_of_bbc(category=None):\n    \"\"\"\n    Gets the corpus of data\n    :param category: The category/class of the the instance\n    :return: The corpus, the length and list of file names\n    \"\"\"\n    files_load = sklearn.datasets.load_files('../data/BBC', description=\"\"\"\n     D. Greene and P. Cunningham. \"Practical Solutions to the Problem of Diagonal Dominance in Kernel Document Clustering\", Proc. ICML 2006.6\n     \"\"\", categories=category, encoding='latin1')\n    length = len(files_load.data)\n    files_names = [fp[fp.find(\"\\\\\") + 1:] for fp in files_load.filenames]\n    return files_load, length, files_names\n\n\n#all BBC data and size\nallBBCFiles, allBBCDataSize, allBBC_filenames = load_files_of_bbc()\n\n# #BBC business Data and size\nbusinessFiles, businessDataSize, business_filenames = load_files_of_bbc('business')\n\n# #BBC entertainment Data and size\nentertainmentFiles, entertainmentDataSize, entertainment_filenames = load_files_of_bbc('entertainment')\n\n# #BBC politics Data and size\npoliticsFiles, politicsDataSize, politics_filenames = load_files_of_bbc('politics')\n\n# #BBC sport Data and size\nsportFiles, sportDataSize, sport_filenames = load_files_of_bbc('sport')\n\n# #BBC tech Data\ntechFiles, techDataSize, tech_filenames = load_files_of_bbc('tech')","metadata":{"id":"VaUqx3yLrIt1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632255366981,"user_tz":240,"elapsed":192,"user":{"displayName":"Ebraheem Al Shapi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJ01BYJAFuUR9CyAFQGAhtsyoXI29CBA4HTAnS-Yg=s64","userId":"03680136128279319983"}},"outputId":"38085c33-bc67-43b8-9cc7-3986fa71afae","pycharm":{"name":"#%%\n"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Step 2: Plot the distribution of the instances in each class and save the graphic in a file called BBC-distribution.pdf. (Task 1.2)","metadata":{}},{"cell_type":"markdown","source":"### Create Dataframe","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nallBBC_DF = pd.DataFrame({\n    'Business': businessDataSize,\n    'Entertainment': entertainmentDataSize,\n    'Politics': politicsDataSize,\n    'Sport': sportDataSize,\n    'Tech': techDataSize\n},\n    index=['Records Count']\n)\n\nallBBC_DF","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"               Business  Entertainment  Politics  Sport  Tech\nRecords Count       510            386       417    511   401","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Business</th>\n      <th>Entertainment</th>\n      <th>Politics</th>\n      <th>Sport</th>\n      <th>Tech</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Records Count</th>\n      <td>510</td>\n      <td>386</td>\n      <td>417</td>\n      <td>511</td>\n      <td>401</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Plot the distribution of the instances in each class","metadata":{}},{"cell_type":"code","source":"% matplotlib inline\n\nsp = allBBC_DF.loc['Records Count'].plot(by=allBBCFiles.target_names, title='Records Count', figsize=(12, 6),\n                                         kind='barh')","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"UsageError: Line magic function `%` not found.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Save the graphic in a file called BBC-distribution.pdf","metadata":{}},{"cell_type":"code","source":"fig = sp.get_figure()\nfig.savefig('../out/BBC-distribution.pdf')","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":7,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-e0bf781801b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../out/BBC-distribution.pdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'sp' is not defined"],"ename":"NameError","evalue":"name 'sp' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"## Step 3: Pre-process the dataset to have the features ready to be used by a multinomial Naive Bayes classifier. (Task 1.4)","metadata":{}},{"cell_type":"markdown","source":"### Prepare the vectorizer","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer()\n\n\ndef get_matrix_and_vocabulary(data):\n    return vectorizer.fit_transform(data).toarray(), {k: v for k, v in\n                                                      sorted(vectorizer.vocabulary_.items(), key=lambda item: item[1])}","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Extract matrices","metadata":{}},{"cell_type":"code","source":"# TODO: purify data\n# creating X and Y for train test split (merging all directories and assigning each entry its proper label\nX = []\nY = []\n\n\nbusinessMatrix, businessVocab = get_matrix_and_vocabulary(businessFiles.data)\nfor businessVector in businessMatrix:\n    X.append(businessVector)\n    Y.append('business')\n\nentertainmentMatrix, entertainmentVocab = get_matrix_and_vocabulary(entertainmentFiles.data)\nfor entertainmentVector in entertainmentMatrix:\n    X.append(entertainmentVector)\n    Y.append('entertainment')\n    \npoliticsMatrix, politicsVocab = get_matrix_and_vocabulary(politicsFiles.data)\nfor politicsVector in politicsMatrix:\n    X.append(politicsVector)\n    Y.append('politics')\n    \nsportMatrix, sportVocab = get_matrix_and_vocabulary(sportFiles.data)\nfor sportVector in sportMatrix:\n    X.append(sportVector)\n    Y.append('sport')\n    \ntechMatrix, techVocab = get_matrix_and_vocabulary(techFiles.data)\nfor techVector in techMatrix:\n    X.append(techVector)\n    Y.append('tech')\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"2225\n2225\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Step 4: Split the dataset into 80% for training and 20% for testing. For this, you must use train test split with the parameter random state set to None. (Task 1.5)","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}